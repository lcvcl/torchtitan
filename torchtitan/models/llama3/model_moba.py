# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.
#
# Copyright (c) Meta Platforms, Inc. All Rights Reserved.

from dataclasses import dataclass

import torch
from torch import nn

from torchtitan.components.tokenizer import Tokenizer
from torchtitan.config_manager import JobConfig
from torchtitan.protocols.train_spec import BaseModelArgs
from torchtitan.models.attention import build_attention, init_attention_mask
from . import model
from .moba.config import MoBAConfig
from .moba.wrapper import moba_layer
from .moba.moba_efficient import moba_attn_varlen

@dataclass
class MoBATransformerModelArgs(model.TransformerModelArgs):
    # Moba specific parameters
    attention_type = "moba"
    moba_chunk_size: int = 64  # Size of each chunk for Moba attention
    moba_topk: int = 8  # Number of top chunks to attend to
    moba_alpha_init: float = 1.0  # Initial value for alpha parameter
    moba_beta_init: float = 1.0  # Initial value for beta parameter
    moba_gamma_init: float = 1.0  # Initial value for gamma parameter
    
    # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæ·»åŠ ç¼–è¯‘ç›¸å…³å‚æ•°
    use_compile: bool = False  # æ˜¯å¦ä½¿ç”¨torch.compileä¼˜åŒ–

    def update_from_config(self, job_config: JobConfig, tokenizer: Tokenizer) -> None:
        super().update_from_config(job_config, tokenizer)
        # Update Moba parameters from config if they exist
        if hasattr(job_config.model, "moba_chunk_size"):
            self.moba_chunk_size = job_config.model.moba_chunk_size
        if hasattr(job_config.model, "moba_topk"):
            self.moba_topk = job_config.model.moba_topk
        if hasattr(job_config.model, "moba_alpha_init"):
            self.moba_alpha_init = job_config.model.moba_alpha_init
        if hasattr(job_config.model, "moba_beta_init"):
            self.moba_beta_init = job_config.model.moba_beta_init
        if hasattr(job_config.model, "moba_gamma_init"):
            self.moba_gamma_init = job_config.model.moba_gamma_init
        if hasattr(job_config.model, "use_compile"):
            self.use_compile = job_config.model.use_compile

class MoBAAttention(model.Attention):
    def __init__(self, model_args: MoBATransformerModelArgs, shared_moba_config: MoBAConfig = None):
        super().__init__(model_args)
        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå…±äº«MoBAé…ç½®ï¼Œé¿å…é‡å¤åˆ›å»º
        if shared_moba_config is not None:
            self.moba_config = shared_moba_config
        else:
            self.moba_config = MoBAConfig(
                moba_chunk_size=model_args.moba_chunk_size,
                moba_topk=model_args.moba_topk
            )
        self.is_causal = True
        
        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜å¸¸ç”¨å€¼ï¼Œé¿å…é‡å¤è®¡ç®—
        self._chunk_size = model_args.moba_chunk_size
        self._topk = model_args.moba_topk

    def forward(
        self,
        x: torch.Tensor,  # [bs, seqlen, dim]
        freqs_cis: torch.Tensor,
    ):
        bs, seqlen, _ = x.shape
        
        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šåˆå¹¶çº¿æ€§å˜æ¢ï¼Œç¡®ä¿å†…å­˜è¿žç»­æ€§
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
        
        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡reshape + contiguousï¼Œå‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
        xq = xq.view(bs, seqlen, -1, self.head_dim).contiguous()
        xk = xk.view(bs, seqlen, -1, self.head_dim).contiguous()
        xv = xv.view(bs, seqlen, -1, self.head_dim).contiguous()
        
        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šRoPEåº”ç”¨åœ¨è¿žç»­å†…å­˜ä¸Šæ›´é«˜æ•ˆ
        xq, xk = model.apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)

        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šç›´æŽ¥ä¼ é€’ç»™moba_layerï¼Œä¸€æ¬¡æ€§transpose + contiguous
        output, _ = moba_layer(
            moba_impl=moba_attn_varlen,
            moba_config=self.moba_config,
            module=self,
            query=xq.transpose(1, 2).contiguous(),  # å†…å­˜è¿žç»­æ€§å¯¹Flash Attentionå¾ˆé‡è¦
            key=xk.transpose(1, 2).contiguous(),
            value=xv.transpose(1, 2).contiguous(),
        )

        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šç›´æŽ¥reshapeè¾“å‡ºï¼Œé¿å…ä¸­é—´å˜é‡
        return self.wo(output.view(bs, seqlen, -1))

class TransformerMoBA(model.Transformer):
    def __init__(self, model_args: MoBATransformerModelArgs):
        super().__init__(model_args)
        
        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šåˆ›å»ºå…±äº«çš„MoBAé…ç½®ï¼Œæ‰€æœ‰layerå¤ç”¨åŒä¸€ä¸ªå¯¹è±¡
        shared_moba_config = MoBAConfig(
            moba_chunk_size=model_args.moba_chunk_size,
            moba_topk=model_args.moba_topk
        )
        
        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šæ‰¹é‡æ›¿æ¢attentionæ¨¡å—ï¼Œä½¿ç”¨å…±äº«é…ç½®
        for layer in self.layers.values():
            layer.attention = MoBAAttention(model_args, shared_moba_config)
            
        # ðŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå¦‚æžœå¯ç”¨ç¼–è¯‘ï¼Œå¯¹attentionæ¨¡å—è¿›è¡Œtorch.compileä¼˜åŒ–
        if model_args.use_compile:
            for layer in self.layers.values():
                if hasattr(torch, 'compile'):  # ç¡®ä¿torch.compileå¯ç”¨
                    layer.attention.forward = torch.compile(layer.attention.forward)

    @classmethod
    def from_model_args(cls, model_args: MoBATransformerModelArgs) -> "TransformerMoBA":
        return cls(model_args) 